"""Wrapper around ZhipuAI APIs."""
from __future__ import annotations

import logging
import posixpath
from typing import (
    Any,
    Dict,
    List,
    Optional, Iterator,
)

import zhipuai
from langchain.llms.utils import enforce_stop_tokens
from langchain.schema.output import GenerationChunk
from pydantic import Extra, root_validator, BaseModel

from langchain.callbacks.manager import (
    CallbackManagerForLLMRun,
)
from langchain.llms.base import LLM
from langchain.utils import get_from_dict_or_env
from zhipuai.model_api.api import InvokeType
from zhipuai.utils import jwt_token
from zhipuai.utils.http_client import post, stream
from zhipuai.utils.sse_client import SSEClient

logger = logging.getLogger(__name__)


class ZhipuModelAPI(BaseModel):
    api_key: str
    api_timeout_seconds = 60

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    def invoke(self, **kwargs):
        url = self._build_api_url(kwargs, InvokeType.SYNC)
        return post(url, self._generate_token(), kwargs, self.api_timeout_seconds)

    def sse_invoke(self, **kwargs):
        url = self._build_api_url(kwargs, InvokeType.SSE)
        data = stream(url, self._generate_token(), kwargs, self.api_timeout_seconds)
        return SSEClient(data)

    def _build_api_url(self, kwargs, *path):
        if kwargs:
            if "model" not in kwargs:
                raise Exception("model param missed")
            model = kwargs.pop("model")
        else:
            model = "-"

        return posixpath.join(zhipuai.model_api_url, model, *path)

    def _generate_token(self):
        if not self.api_key:
            raise Exception(
                "api_key not provided, you could provide it."
            )

        return jwt_token.generate_token(self.api_key)


class ZhipuAIChatLLM(LLM):
    """Wrapper around ZhipuAI large language models.
    To use, you should pass the api_key as a named parameter to the constructor.
    Example:
     .. code-block:: python
         from core.third_party.langchain.llms.zhipuai import ZhipuAI
         model = ZhipuAI(model="<model_name>", api_key="my-api-key")
    """

    client: Any
    model: str = "chatglm_lite"
    """Model name to use."""
    temperature: float = 0.95
    """A non-negative float that tunes the degree of randomness in generation."""
    top_p: float = 0.7
    """Total probability mass of tokens to consider at each step."""
    streaming: bool = False
    """Whether to stream the response or return it all at once."""
    api_key: Optional[str] = None

    base_url: str = "https://open.bigmodel.cn/api/paas/v3/model-api/{model}/{invoke_method}"

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that api key and python package exists in environment."""
        values["api_key"] = get_from_dict_or_env(
            values, "api_key", "ZHIPUAI_API_KEY"
        )
        values['client'] = ZhipuModelAPI(api_key=values['api_key'])
        return values

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        return {
            "model": self.model,
            "temperature": self.temperature,
            "top_p": self.top_p
        }

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return self._default_params

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "zhipuai"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        r"""Call out to ZhupuAI's completion endpoint to chat
        Args:
            prompt: The prompt to pass into the model.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = model("Tell me a joke.")
        """
        if self.streaming:
            completion = ""
            for chunk in self._stream(
                prompt=prompt, stop=stop, run_manager=run_manager, **kwargs
            ):
                completion += chunk.text
        else:
            request = self._default_params
            request["prompt"] = [{"role": "user", "content": prompt}]
            request.update(kwargs)
            result = self.client.invoke(**request)
            completion = result['data']['choices'][0]['content']
            token_usage = result['data']['usage']

        if stop is not None:
            completion = enforce_stop_tokens(completion, stop)

        return completion

    def _stream(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[GenerationChunk]:
        r"""Call ZhipuAI completion_stream and return the resulting generator.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            A generator representing the stream of tokens from Wenxin.
        Example:
            .. code-block:: python

                prompt = "Write a poem about a stream."
                prompt = f"\n\nHuman: {prompt}\n\nAssistant:"
                generator = model.stream(prompt)
                for token in generator:
                    yield token
        """
        request = self._default_params
        request["prompt"] = [{"role": "user", "content": prompt}]
        request.update(kwargs)

        for event in self.client.sse_invoke(incremental=True, **request).events():
            if event.event == "add":
                yield GenerationChunk(text=event.data)
                if run_manager:
                    run_manager.on_llm_new_token(event.data)
            elif event.event == "error" or event.event == "interrupted":
                raise ValueError(
                    f"ZhipuAI API error: {event.data}"
                )
            elif event.event == "finish":
                print(event.data)
                print(event.meta)
                token_usage = event.meta['usage']
            else:
                print(event.data)
